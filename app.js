// app.js (GPT + í‘œì • + í–‰ë™ ë¶„ì„ í†µí•©)

const questions = [
  "ìžê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”.",
  "ìµœê·¼ì— ë„ì „í•œ ê²½í—˜ì— ëŒ€í•´ ë§í•´ì£¼ì„¸ìš”.",
  "ê°ˆë“±ì„ í•´ê²°í•œ ê²½í—˜ì´ ìžˆë‚˜ìš”?",
  "ì§€ì›í•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
];

let currentQuestion = "";

// Load face-api.js models
Promise.all([
  faceapi.nets.tinyFaceDetector.loadFromUri('/models'),
  faceapi.nets.faceExpressionNet.loadFromUri('/models')
]).then(startVideo);

function startVideo() {
  navigator.mediaDevices.getUserMedia({ video: {} })
    .then(stream => video.srcObject = stream)
    .catch(console.error);
}

const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
const recognition = new SpeechRecognition();
recognition.lang = 'ko-KR';
recognition.interimResults = false;

const startBtn = document.getElementById("startBtn");
const prompt = document.getElementById("prompt");
const feedbackDiv = document.getElementById("feedback");
const questionBox = document.getElementById("questionBox");
const video = document.getElementById("video");

startBtn.addEventListener("click", () => {
  startBtn.style.display = "none";
  feedbackDiv.innerText = "";
  currentQuestion = pickQuestion();
  questionBox.innerText = `ðŸ’¬ ${currentQuestion}`;
  prompt.innerText = "ë‹µë³€ì„ ì‹œìž‘í•´ì£¼ì„¸ìš”...";
  recognition.start();
});

recognition.onresult = async (e) => {
  const text = e.results[0][0].transcript;
  prompt.innerText = "ë‹µë³€ ë¶„ì„ ì¤‘...";

  // í‘œì •
  const expr = await faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions()).withFaceExpressions();
  const happyScore = expr?.expressions.happy || 0;

  // ë„ë•ìž„
  const nodded = await observeGesture();

  // GPT ë¶„ì„
  const gpt = await gptFeedback(currentQuestion, text);

  let feedback = `ë‹µë³€: \"${text}\"\n`;
  feedback += `GPT í”¼ë“œë°±: ${gpt.feedback}\n`;
  feedback += `GPT ì ìˆ˜: ${gpt.score}/10\n`;
  feedback += happyScore > 0.6 ? "í‘œì •: ë°ìŒ\n" : "í‘œì •: ë¬´í‘œì •\n";
  feedback += nodded ? "í–‰ë™: ê³ ê°œ ë„ë•ìž„ ê°ì§€ë¨" : "í–‰ë™: ë„ë•ìž„ ì—†ìŒ";

  provideFeedback(feedback);
  resetUI();
};

function pickQuestion() {
  return questions[Math.floor(Math.random() * questions.length)];
}

function provideFeedback(msg) {
  feedbackDiv.innerText = msg;
  const utter = new SpeechSynthesisUtterance(msg);
  utter.lang = 'ko-KR';
  speechSynthesis.speak(utter);
}

function resetUI() {
  startBtn.style.display = "inline-block";
  prompt.innerText = "Start ë²„íŠ¼ì„ ëˆŒëŸ¬ ë‹¤ìŒ ì§ˆë¬¸ì„ ë°›ì•„ë³´ì„¸ìš”.";
}

let yHistory = [];
async function observeGesture() {
  return new Promise((resolve) => {
    yHistory = [];
    const start = performance.now();
    const duration = 3000;

    const interval = setInterval(async () => {
      const result = await faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions());
      if (result?.box) yHistory.push(result.box.top);
      if (performance.now() - start > duration) {
        clearInterval(interval);
        const delta = Math.max(...yHistory) - Math.min(...yHistory);
        resolve(delta > 20);
      }
    }, 150);
  });
}

async function gptFeedback(question, answer) {
  try {
    const res = await fetch("/.netlify/functions/analyze", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ question, answer }),
    });

    if (!res.ok) {
      const errorText = await res.text();
      console.error("API error response:", errorText);
      return { feedback: "GPT API ì˜¤ë¥˜", score: 0 };
    }

    const data = await res.json();
    console.log("GPT response:", data);
    return data;
  } catch (err) {
    console.error("gptFeedback error:", err);
    return { feedback: "GPT í˜¸ì¶œ ì‹¤íŒ¨", score: 0 };
  }
}
